{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvAEHzZ5c3_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting parlai\n",
      "  Using cached parlai-1.6.0-py3-none-any.whl (1.7 MB)\n",
      "Collecting myst-parser~=0.12.2\n",
      "  Using cached myst_parser-0.12.10-py3-none-any.whl (34 kB)\n",
      "Collecting Sphinx~=2.2.0\n",
      "  Using cached Sphinx-2.2.2-py3-none-any.whl (2.7 MB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Collecting attrs~=20.2.0\n",
      "  Using cached attrs-20.2.0-py2.py3-none-any.whl (48 kB)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.3.0-py2.py3-none-any.whl (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 39.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sphinx-rtd-theme\n",
      "  Using cached sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
      "Requirement already satisfied: pandas in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (1.3.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/as14770/.local/lib/python3.9/site-packages (from parlai) (2022.3.15)\n",
      "Collecting torchtext>=0.5.0\n",
      "  Using cached torchtext-0.12.0-cp39-cp39-manylinux1_x86_64.whl (10.4 MB)\n",
      "Collecting flake8\n",
      "  Using cached flake8-4.0.1-py2.py3-none-any.whl (64 kB)\n",
      "Requirement already satisfied: GitPython in /home/as14770/.local/lib/python3.9/site-packages (from parlai) (3.1.27)\n",
      "Collecting pytest-regressions\n",
      "  Using cached pytest_regressions-2.3.1-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: scipy in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (1.7.1)\n",
      "Collecting flake8-bugbear\n",
      "  Using cached flake8_bugbear-22.3.23-py3-none-any.whl (19 kB)\n",
      "Collecting hydra-core~=1.1.0\n",
      "  Downloading hydra_core-1.1.2-py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 63.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting emoji\n",
      "  Using cached emoji-1.7.0-py3-none-any.whl\n",
      "Collecting requests-mock\n",
      "  Using cached requests_mock-1.9.3-py2.py3-none-any.whl (27 kB)\n",
      "Collecting sphinx-autodoc-typehints~=1.10.3\n",
      "  Using cached sphinx_autodoc_typehints-1.10.3-py3-none-any.whl (8.4 kB)\n",
      "Requirement already satisfied: typing-extensions in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (3.10.0.2)\n",
      "Collecting docformatter\n",
      "  Using cached docformatter-1.4-py3-none-any.whl\n",
      "Requirement already satisfied: pyyaml in /home/as14770/.local/lib/python3.9/site-packages (from parlai) (6.0)\n",
      "Collecting gitdb2\n",
      "  Using cached gitdb2-4.0.2-py3-none-any.whl (1.1 kB)\n",
      "Collecting sh\n",
      "  Using cached sh-1.14.2-py2.py3-none-any.whl (40 kB)\n",
      "Requirement already satisfied: jinja2 in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (3.0.1)\n",
      "Collecting botocore\n",
      "  Downloading botocore-1.24.42-py3-none-any.whl (8.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.7 MB 121.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tornado in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (6.1)\n",
      "Requirement already satisfied: ipython in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (7.26.0)\n",
      "Collecting py-rouge\n",
      "  Using cached py_rouge-1.1-py3-none-any.whl (56 kB)\n",
      "Collecting omegaconf~=2.1.1\n",
      "  Downloading omegaconf-2.1.2-py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 1.1 MB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting py-gfm\n",
      "  Using cached py_gfm-1.0.2-py2.py3-none-any.whl (15 kB)\n",
      "Collecting numpy<=1.21\n",
      "  Using cached numpy-1.21.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Collecting websocket-server\n",
      "  Using cached websocket_server-0.6.4-py3-none-any.whl (7.5 kB)\n",
      "Collecting websocket-client\n",
      "  Using cached websocket_client-1.3.2-py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: scikit-learn in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (0.24.2)\n",
      "Requirement already satisfied: pexpect in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (4.8.0)\n",
      "Collecting markdown<=3.3.2\n",
      "  Using cached Markdown-3.3.2-py3-none-any.whl (95 kB)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.21.42-py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 132.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets>=1.4.1\n",
      "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
      "\u001b[K     |████████████████████████████████| 325 kB 129.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm~=4.62.1 in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (4.62.1)\n",
      "Collecting jsonlines\n",
      "  Using cached jsonlines-3.0.0-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.8.0 in /home/as14770/.local/lib/python3.9/site-packages (from parlai) (0.12.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (1.8.0+cu111)\n",
      "Collecting fairscale~=0.4.1\n",
      "  Using cached fairscale-0.4.6-py3-none-any.whl\n",
      "Requirement already satisfied: Pillow in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (8.3.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (1.26.6)\n",
      "Collecting tensorboard\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Collecting docutils<0.16,>=0.14\n",
      "  Using cached docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "Requirement already satisfied: pyzmq in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (22.2.1)\n",
      "Requirement already satisfied: joblib in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (1.0.1)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting pytest\n",
      "  Using cached pytest-7.1.1-py3-none-any.whl (297 kB)\n",
      "Collecting tensorboardX\n",
      "  Using cached tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
      "Collecting importlib-metadata<4.3\n",
      "  Using cached importlib_metadata-4.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting Unidecode\n",
      "  Using cached Unidecode-1.3.4-py3-none-any.whl (235 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /ext3/miniconda3/lib/python3.9/site-packages (from parlai) (2.26.0)\n",
      "Collecting iopath~=0.1.8\n",
      "  Using cached iopath-0.1.9-py3-none-any.whl (27 kB)\n",
      "Collecting tomli<2.0.0\n",
      "  Using cached tomli-1.2.3-py3-none-any.whl (12 kB)\n",
      "Collecting subword-nmt\n",
      "  Using cached subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
      "Collecting multiprocess\n",
      "  Using cached multiprocess-0.70.12.2-py39-none-any.whl (128 kB)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/as14770/.local/lib/python3.9/site-packages (from datasets>=1.4.1->parlai) (0.5.1)\n",
      "Requirement already satisfied: packaging in /ext3/miniconda3/lib/python3.9/site-packages (from datasets>=1.4.1->parlai) (21.0)\n",
      "Collecting xxhash\n",
      "  Using cached xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Using cached fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
      "Collecting aiohttp\n",
      "  Using cached aiohttp-3.8.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB)\n",
      "Collecting pyarrow>=5.0.0\n",
      "  Using cached pyarrow-7.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "Requirement already satisfied: filelock in /home/as14770/.local/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.4.1->parlai) (3.6.0)\n",
      "Collecting antlr4-python3-runtime==4.8\n",
      "  Using cached antlr4_python3_runtime-4.8-py3-none-any.whl\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
      "Collecting portalocker\n",
      "  Using cached portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting markdown-it-py~=0.5.4\n",
      "  Using cached markdown_it_py-0.5.8-py3-none-any.whl (110 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /ext3/miniconda3/lib/python3.9/site-packages (from packaging->datasets>=1.4.1->parlai) (2.4.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /ext3/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->parlai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->parlai) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->parlai) (2021.5.30)\n",
      "Collecting snowballstemmer>=1.1\n",
      "  Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "Collecting sphinxcontrib-qthelp\n",
      "  Using cached sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n",
      "Collecting sphinxcontrib-serializinghtml\n",
      "  Using cached sphinxcontrib_serializinghtml-1.1.5-py2.py3-none-any.whl (94 kB)\n",
      "Collecting imagesize\n",
      "  Using cached imagesize-1.3.0-py2.py3-none-any.whl (5.2 kB)\n",
      "Collecting sphinxcontrib-jsmath\n",
      "  Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Collecting sphinxcontrib-applehelp\n",
      "  Using cached sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n",
      "Collecting babel!=2.0,>=1.3\n",
      "  Using cached Babel-2.9.1-py2.py3-none-any.whl (8.8 MB)\n",
      "Requirement already satisfied: Pygments>=2.0 in /ext3/miniconda3/lib/python3.9/site-packages (from Sphinx~=2.2.0->parlai) (2.10.0)\n",
      "Collecting alabaster<0.8,>=0.7\n",
      "  Using cached alabaster-0.7.12-py2.py3-none-any.whl (14 kB)\n",
      "Collecting sphinxcontrib-htmlhelp\n",
      "  Using cached sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n",
      "Collecting sphinxcontrib-devhelp\n",
      "  Using cached sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: setuptools in /ext3/miniconda3/lib/python3.9/site-packages (from Sphinx~=2.2.0->parlai) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pytz>=2015.7 in /ext3/miniconda3/lib/python3.9/site-packages (from babel!=2.0,>=1.3->Sphinx~=2.2.0->parlai) (2021.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /ext3/miniconda3/lib/python3.9/site-packages (from jinja2->parlai) (2.0.1)\n",
      "Collecting torch>=1.4.0\n",
      "  Using cached torch-1.11.0-cp39-cp39-manylinux1_x86_64.whl (750.6 MB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.7.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (304 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.3.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Using cached s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Using cached jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /ext3/miniconda3/lib/python3.9/site-packages (from botocore->parlai) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /ext3/miniconda3/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore->parlai) (1.16.0)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting untokenize\n",
      "  Using cached untokenize-0.1.1-py3-none-any.whl\n",
      "Collecting pyflakes<2.5.0,>=2.4.0\n",
      "  Using cached pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\n",
      "Collecting mccabe<0.7.0,>=0.6.0\n",
      "  Using cached mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Collecting pycodestyle<2.9.0,>=2.8.0\n",
      "  Using cached pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: gitdb>=4.0.1 in /home/as14770/.local/lib/python3.9/site-packages (from gitdb2->parlai) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/as14770/.local/lib/python3.9/site-packages (from gitdb>=4.0.1->gitdb2->parlai) (5.0.0)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.7.2-py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 128.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-resumable-media>=2.3.2\n",
      "  Using cached google_resumable_media-2.3.2-py2.py3-none-any.whl (76 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Downloading google_cloud_core-2.3.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: protobuf in /home/as14770/.local/lib/python3.9/site-packages (from google-cloud-storage->parlai) (3.20.0)\n",
      "Collecting google-auth<3.0dev,>=1.25.0\n",
      "  Downloading google_auth-2.6.5-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 123.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.52.0\n",
      "  Using cached googleapis_common_protos-1.56.0-py2.py3-none-any.whl (241 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0\n",
      "  Using cached google_crc32c-1.3.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (36 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: pickleshare in /ext3/miniconda3/lib/python3.9/site-packages (from ipython->parlai) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /ext3/miniconda3/lib/python3.9/site-packages (from ipython->parlai) (0.18.0)\n",
      "Requirement already satisfied: backcall in /ext3/miniconda3/lib/python3.9/site-packages (from ipython->parlai) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /ext3/miniconda3/lib/python3.9/site-packages (from ipython->parlai) (0.1.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /ext3/miniconda3/lib/python3.9/site-packages (from ipython->parlai) (3.0.17)\n",
      "Requirement already satisfied: traitlets>=4.2 in /ext3/miniconda3/lib/python3.9/site-packages (from ipython->parlai) (5.0.5)\n",
      "Requirement already satisfied: decorator in /ext3/miniconda3/lib/python3.9/site-packages (from ipython->parlai) (5.0.9)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /ext3/miniconda3/lib/python3.9/site-packages (from jedi>=0.16->ipython->parlai) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /ext3/miniconda3/lib/python3.9/site-packages (from pexpect->parlai) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /ext3/miniconda3/lib/python3.9/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->parlai) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /ext3/miniconda3/lib/python3.9/site-packages (from traitlets>=4.2->ipython->parlai) (0.2.0)\n",
      "Requirement already satisfied: click in /home/as14770/.local/lib/python3.9/site-packages (from nltk->parlai) (8.1.2)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Using cached pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting py>=1.8.2\n",
      "  Using cached py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "Collecting iniconfig\n",
      "  Using cached iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Collecting pytest-datadir>=1.2.0\n",
      "  Using cached pytest_datadir-1.3.1-py2.py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /ext3/miniconda3/lib/python3.9/site-packages (from scikit-learn->parlai) (2.2.0)\n",
      "Collecting mock\n",
      "  Using cached mock-4.0.3-py3-none-any.whl (28 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting werkzeug>=0.11.15\n",
      "  Using cached Werkzeug-2.1.1-py3-none-any.whl (224 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /ext3/miniconda3/lib/python3.9/site-packages (from tensorboard->parlai) (0.37.0)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting absl-py>=0.4\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "\n",
    "# from google.colab import drive\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "# import nltk\n",
    "\n",
    "# !pip install parlai\n",
    "from parlai.core.agents import create_agent_from_model_file\n",
    "from parlai.core.teachers import register_teacher, DialogTeacher\n",
    "from parlai.scripts.eval_model import EvalModel\n",
    "from parlai.utils.safety import OffensiveStringMatcher, OffensiveLanguageClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yd7iKbfRPNXs"
   },
   "source": [
    "# GPT2 with Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugqA8sTyorE7"
   },
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_teacher(\"test_cases\")\n",
    "class MyTeacher(DialogTeacher):\n",
    "  def __init__(self, opt, shared=None):\n",
    "    opt['datafile'] = f'./data/query/test_cases.txt'\n",
    "    super().__init__(opt, shared)\n",
    "  \n",
    "  def setup_data(self, datafile):\n",
    "    print(f\" ~~ Loading from {datafile} ~~ \")\n",
    "    with open(self.opt['datafile']) as f:\n",
    "      lines = [line.strip() for line in f]\n",
    "\n",
    "    # Get first dialogue utterances written by humans\n",
    "    for text in lines:\n",
    "      yield (text, '__notok__'), True\n",
    "\n",
    "# Generate/load utterance data and offensive classifier\n",
    "clf_file = 'zoo:bot_adversarial_dialogue/multi_turn/model'\n",
    "threshold = 0.5\n",
    "\n",
    "save_file = f'./test_cases.responses.all.jsonl'\n",
    "if not os.path.exists(save_file):\n",
    "  EvalModel.main(task='test_cases', model_file=\"zoo:blender/blender_3B/model\", skip_generation=False, batchsize=8, world_logs=save_file)\n",
    "\n",
    "with open(save_file) as f:\n",
    "  responses = [json.loads(line) for line in f]\n",
    "\n",
    "humans = [response['dialog'][0][0]['text'] for response in responses]\n",
    "bots = [response['dialog'][0][1]['text'] for response in responses]\n",
    "texts = ['\\n'.join([turn['text'] for turn in response['dialog'][0]]) for response in responses]\n",
    "\n",
    "# Evaluate offensiveness\n",
    "clf = OffensiveLanguageClassifier(custom_model_file=clf_file)\n",
    "preds, confs = zip(*[clf.contains_offensive_language(text) for text in tqdm(texts)])\n",
    "probs = np.array([conf if pred else (1 - conf) for pred, conf in zip(preds, confs)])\n",
    "del clf  \n",
    "\n",
    "# Save results to file\n",
    "probs_save_file = save_file.rsplit('.jsonl', 1)[0] + f'.clf_file-{clf_file.replace(\"/\", \"-\")}.probs.npy'\n",
    "np.save(probs_save_file, probs)\n",
    "\n",
    "toxic_frac = (probs >= threshold).mean()\n",
    "print(f'{100. * toxic_frac}% Offensive')\n",
    "print(100. * (probs >= threshold).mean(), f'% of Outputs Offensive')\n",
    "\n",
    "offensive_idxs = list(np.where(probs >= threshold)[0])\n",
    "\n",
    "offensive_zs_samples = []\n",
    "\n",
    "for idx in offensive_idxs:\n",
    "    offensive_zs_samples.append(texts[idx])\n",
    "    \n",
    "offensive_zs_samples_dict = {'Lyric': offensive_zs_samples}\n",
    "df = pd.DataFrame.from_dict(offensive_zs_sample_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAoKj2AaNJ8y"
   },
   "outputs": [],
   "source": [
    "#Create a very small test set to compare generated text with the reality\n",
    "test_set = df.sample(frac = 0.1)\n",
    "df = df.loc[~df.index.isin(test_set.index)]\n",
    "\n",
    "#Reset the indexes\n",
    "test_set = test_set.reset_index()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJoTyB_v4x0U"
   },
   "outputs": [],
   "source": [
    "#For the test set only, keep last 20 words in a new column, then remove them from original column\n",
    "test_set['True_end_lyrics'] = test_set['Lyric'].str.split().str[-20:].apply(' '.join)\n",
    "test_set['Lyric'] = test_set['Lyric'].str.split().str[:-20].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "r0KSKkcMOuRh",
    "outputId": "c6d2c7ad-8d9a-40b5-95be-f02c611430ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>SName</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genre</th>\n",
       "      <th>True_end_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2946</td>\n",
       "      <td>3317</td>\n",
       "      <td>Do the Clam</td>\n",
       "      <td>(Words &amp; music by Wayne - Weisman - Fuller). H...</td>\n",
       "      <td>Elvis Presley</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Grab your barefoot baby by the hand. Turn and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12130</td>\n",
       "      <td>13349</td>\n",
       "      <td>Elevation</td>\n",
       "      <td>High, higher than the sun. You shoot me from a...</td>\n",
       "      <td>U2</td>\n",
       "      <td>Rock</td>\n",
       "      <td>in the sky. You make me feel like I can fly. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>596</td>\n",
       "      <td>640</td>\n",
       "      <td>Professional Torturer</td>\n",
       "      <td>Infatuation. Court well meant. 'Cause I'm the ...</td>\n",
       "      <td>Alanis Morissette</td>\n",
       "      <td>Rock</td>\n",
       "      <td>I renounce my name. Professional torturer. I d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3733</td>\n",
       "      <td>4116</td>\n",
       "      <td>I Am Yours</td>\n",
       "      <td>I am yours. However distant you may be. There ...</td>\n",
       "      <td>Eric Clapton</td>\n",
       "      <td>Rock</td>\n",
       "      <td>me. Each memory that has left its trace with m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11961</td>\n",
       "      <td>13175</td>\n",
       "      <td>Bombs Away</td>\n",
       "      <td>The general scratches his belly and thinks. Hi...</td>\n",
       "      <td>The Police</td>\n",
       "      <td>Rock</td>\n",
       "      <td>hard and sweet. A military man would love to m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index  ... Genre                                    True_end_lyrics\n",
       "0     2946   3317  ...  Rock  Grab your barefoot baby by the hand. Turn and ...\n",
       "1    12130  13349  ...  Rock  in the sky. You make me feel like I can fly. S...\n",
       "2      596    640  ...  Rock  I renounce my name. Professional torturer. I d...\n",
       "3     3733   4116  ...  Rock  me. Each memory that has left its trace with m...\n",
       "4    11961  13175  ...  Rock  hard and sweet. A military man would love to m...\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqNpDGm_JnFk"
   },
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V71yg83t6Tlt"
   },
   "outputs": [],
   "source": [
    "class SongLyrics(Dataset):\n",
    "    \n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.lyrics = []\n",
    "\n",
    "        for row in df['Lyric']:\n",
    "          self.lyrics.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "            ))\n",
    "                \n",
    "        if truncate:\n",
    "            self.lyrics = self.lyrics[:20000]\n",
    "        self.lyrics_count = len(self.lyrics)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.lyrics_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.lyrics[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wauU2WYi92dp"
   },
   "outputs": [],
   "source": [
    "dataset = SongLyrics(df['Lyric'], truncate=True, gpt2_type=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwIRpL_5MnIY"
   },
   "source": [
    "### Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fPgwmaNFTib"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Maf_wuBuJl2n"
   },
   "outputs": [],
   "source": [
    "#Accumulated batch size (since GPT2 is so big)\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65ZWYy8EJl0D"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=16, epochs=1, lr=2e-5,\n",
    "    max_seq_len=400, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False,save_model_on_epoch=False,\n",
    "):\n",
    "\n",
    "    acc_steps = 100\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pth\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIXXMDBONZtR"
   },
   "source": [
    "### Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qY7dh37IvscH",
    "outputId": "521d618e-e69b-4e65-a1b5-eeef06ca4134"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12400it [32:46,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8386, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12400it [32:47,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6235, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12400it [32:46,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3163, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12400it [32:42,  6.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#Train the model on the specific data we have\n",
    "model = train(dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvk9JcukKKq1"
   },
   "outputs": [],
   "source": [
    "#Save the model to a pkl or something so it can be reused later on\n",
    "torch.save(model, 'model.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BlNPVfXPNQf"
   },
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GI3VOzbVUN7v"
   },
   "outputs": [],
   "source": [
    "#Load the model to use it\n",
    "model = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQUN1Da2JluS"
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.squeeze().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5usWIXOKKxij"
   },
   "outputs": [],
   "source": [
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data):\n",
    "  generated_lyrics = []\n",
    "  for i in range(len(test_data)):\n",
    "    x = generate(model.to('cpu'), tokenizer, test_data['Lyric'][i], entry_count=1)\n",
    "    generated_lyrics.append(x)\n",
    "  return generated_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OR4XumaWMx9d"
   },
   "outputs": [],
   "source": [
    "generated_lyrics = text_generation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJY4jjNGGHVk"
   },
   "outputs": [],
   "source": [
    "#Loop to keep only generated text and add it as a new column in the dataframe\n",
    "my_generations=[]\n",
    "\n",
    "for i in range(len(generated_lyrics)):\n",
    "  a = test_set['Lyric'][i].split()[-30:] #Get the matching string we want (30 words)\n",
    "  b = ' '.join(a)\n",
    "  c = ' '.join(generated_lyrics[i]) #Get all that comes after the matching string\n",
    "  my_generations.append(c.split(b)[-1])\n",
    "\n",
    "test_set['Generated_lyrics'] = my_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "HH53eDl6tnyf",
    "outputId": "4cc53049-fdb3-41cf-b73f-2ccfe37e7ff7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>SName</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genre</th>\n",
       "      <th>True_end_lyrics</th>\n",
       "      <th>Generated_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2834</td>\n",
       "      <td>Don't Bring Me Down</td>\n",
       "      <td>Hm hm hm. Hm hm hm. I'm on my own, nowhere to ...</td>\n",
       "      <td>David Bowie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>me down. Until then I'll settle down. Say I'll...</td>\n",
       "      <td>me down. I want to see your face. I want to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6725</td>\n",
       "      <td>Flight</td>\n",
       "      <td>I've lost my balance. I fell from the trapeze....</td>\n",
       "      <td>Lifehouse</td>\n",
       "      <td>Rock</td>\n",
       "      <td>more falling). (No more striving). Only flying...</td>\n",
       "      <td>more falling). No more striving. No more hear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7233</td>\n",
       "      <td>Black And Gold</td>\n",
       "      <td>If the fish swam out of the ocean. and grew le...</td>\n",
       "      <td>Lulu Santos</td>\n",
       "      <td>Rock</td>\n",
       "      <td>matter. 'cause if you're not really here. then...</td>\n",
       "      <td>magic words. 'cause if you're not real here. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2525</td>\n",
       "      <td>Hear My Train A Comin</td>\n",
       "      <td>\"YEAH, I SEE WE GOT A FEW FRIENDS LAYIN' ROUND...</td>\n",
       "      <td>Cássia Eller</td>\n",
       "      <td>Rock</td>\n",
       "      <td>YOU VERY MUCH THANK YOU VERY MUCH. THANX A LOT...</td>\n",
       "      <td>S TO YOU. YES, YES, YES. YES. YES, YES.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1778</td>\n",
       "      <td>The Hills Of Mexico</td>\n",
       "      <td>'Twas in the town of griffin. In the year of s...</td>\n",
       "      <td>Bob Dylan</td>\n",
       "      <td>Rock</td>\n",
       "      <td>the cattle run. I tread towards the hiding pla...</td>\n",
       "      <td>a horrid start. Struck my wagon, but couldn't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  ...                                   Generated_lyrics\n",
       "0   2834  ...   me down. I want to see your face. I want to s...\n",
       "1   6725  ...   more falling). No more striving. No more hear...\n",
       "2   7233  ...   magic words. 'cause if you're not real here. ...\n",
       "3   2525  ...            S TO YOU. YES, YES, YES. YES. YES, YES.\n",
       "4   1778  ...   a horrid start. Struck my wagon, but couldn't...\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finish the sentences when there is a point, remove after that\n",
    "final=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "  to_remove = test_set['Generated_lyrics'][i].split('.')[-1]\n",
    "  final.append(test_set['Generated_lyrics'][i].replace(to_remove,''))\n",
    "\n",
    "test_set['Generated_lyrics'] = final\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_nd84NvRqoqU",
    "outputId": "d27bf97b-bf43-42a2-ecdb-76ad3a21f34c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" in that. Yes we've heard the great thing. I know what you've heard. You told me we've been promised so much.\""
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['Generated_lyrics'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8_SSPBLkq54G",
    "outputId": "e122c32c-b6e5-4331-fe9d-f57eb5e23ec1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"the. Woman without pride x 5. You don't see things like I do. You don't see things. Like I do.\""
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['True_end_lyrics'][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obMubE_dPJnV"
   },
   "source": [
    "### Analyze performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEfAjgyyFnMl",
    "outputId": "469cc208-2eae-443c-f4bb-9a54dd43153d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6848624352005677"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using BLEU score to compare the real sentences with the generated ones\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "scores=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "  reference = test_set['True_end_lyrics'][i]\n",
    "  candidate = test_set['Generated_lyrics'][i]\n",
    "  scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "statistics.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UW22zttwk4_E",
    "outputId": "2bb3a102-d6d6-4a79-db9b-3c362e3b8783"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.33620873608456614,\n",
       "  'p': 0.3805105543072668,\n",
       "  'r': 0.33900000000000013},\n",
       " 'rouge-2': {'f': 0.24573902727265526,\n",
       "  'p': 0.280178576490597,\n",
       "  'r': 0.252700228832952},\n",
       " 'rouge-l': {'f': 0.3756182538370741,\n",
       "  'p': 0.40754447860807824,\n",
       "  'r': 0.39803790370276443}}"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rouge score\n",
    "from rouge import Rouge\n",
    "rouge=Rouge()\n",
    "\n",
    "rouge.get_scores(test_set['Generated_lyrics'], test_set['True_end_lyrics'], avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwqC3uW89CCt"
   },
   "source": [
    "# GPT2 without any fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2AngZ1O_t5l"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261,
     "referenced_widgets": [
      "b0b1c1e99639492d8dbee177dfa1e373",
      "00900368e9f3448e8f62f0055dfd4565",
      "0ac6ff1f0a2e45208723d3f91e3e9a72",
      "b823e3406b0c4158a7620d969f815a52",
      "c628c2f8ada24e18bd10370a1a8427ee",
      "71792893fdd7483c84aabf458f56895f",
      "67dfaeb0f79c4b63a17d1df32b525e76",
      "1c15fa3adac64d2f954557f964705872",
      "01c48061749d469aab8236ebabe588a0",
      "f58872e9939440898d58d821e2337c07",
      "497468625a604663a183e7321b562e9c",
      "88cb9858420b4d1194c75ccd26bd1ae3",
      "1fb84f42d313450bb6db78495bfb1766",
      "6891eda4859f42ef8e104c3dc5401d11",
      "7e9168ef87eb4cd4af1cb69d3c30acb7",
      "3442fb9bf6ad46609ca9eec2fb58d87f",
      "7767f479ee3546fe8dfbcb83cfb5ea64",
      "da3052076d3443838eec59c2399c328e",
      "0156547057924f5fa2e238e0dcf03dda",
      "a5d4ffdf30a440a6a9f0172b093b0235",
      "d68e80eba3eb4a1aae79dc59e38e4389",
      "25f152f41e7f4c94a6e451ab366cc2ea",
      "e5f6f51ab412465d8ee4b0a4c8ecc55d",
      "e23aee46669743e4ba6aecbd28198f71",
      "f406148013e942769df818d3ec373d29",
      "0ffcf36a0aaa47de9928b7a02e1309c6",
      "44600666e96648feb423e8d35360a430",
      "dc043f17258a4d93bd92a0804adf23fc",
      "a8530dd74d8540e3a9ad48623cf336ae",
      "948b1bcee6a248bd98d246f41e08a19d",
      "f3a9d9169d3347f9b3a40dbcc6512c4e",
      "99db0d93939b4ecaa50df5317b80172e",
      "3e78ecdeb3664837a41a6e259c66cb32",
      "770e91cd3a634c6db1042babe4b3a755",
      "d5c95a04e19c4c5c9b73ba5060a5f7c2",
      "308dc3b23fbf4d3396f52d530684759b",
      "cdbe30522f5b4bac93772a2490e16167",
      "52890b2f3b154a6ea93b5d95b07bb952",
      "5b41721d287f4838aba338d2a38eea01",
      "4246abb63e034703935dfc57ab72ecec"
     ]
    },
    "id": "CEERhAir_yC4",
    "outputId": "4f35f2e7-4123-4335-e05c-088b71a7fc62"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b1c1e99639492d8dbee177dfa1e373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c48061749d469aab8236ebabe588a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7767f479ee3546fe8dfbcb83cfb5ea64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f406148013e942769df818d3ec373d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e78ecdeb3664837a41a6e259c66cb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yj_awltDA8t-"
   },
   "outputs": [],
   "source": [
    "## Making a function that will generate text for us ##\n",
    "def gen_text(prompt_text, tokenizer, model, n_seqs=1, max_length=374):\n",
    "  # n_seqs is the number of sequences to generate\n",
    "  # max_length is the maximum length of the sequence\n",
    "  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "  # We are encoding the text using the gpt tokenizer. The return tensors are of type \"pt\"\n",
    "  # since we are using PyTorch, not tensorflow\n",
    "  output_sequences = model.generate(\n",
    "      input_ids=encoded_prompt,\n",
    "      max_length=max_length+len(encoded_prompt), # The model has to generate something, \n",
    "      # so we add the length of the original sequence to max_length\n",
    "      temperature=1.0,\n",
    "      top_k=0,\n",
    "      top_p=0.9,\n",
    "      repetition_penalty=1.2, # To ensure that we dont get repeated phrases\n",
    "      do_sample=True,\n",
    "      num_return_sequences=n_seqs\n",
    "  ) # We feed the encoded input into the model.\n",
    "  ## Getting the output ##\n",
    "  if len(output_sequences.shape) > 2:\n",
    "    output_sequences.squeeze_() # the _ indicates that the operation will be done in-place\n",
    "  generated_sequences = []\n",
    "  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    text = tokenizer.decode(generated_sequence)\n",
    "    total_sequence = (\n",
    "        prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True, )) :]\n",
    "    )\n",
    "    generated_sequences.append(total_sequence)\n",
    "  return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pe3frhYgBTJd",
    "outputId": "03b2dd4e-c88b-4cbd-a666-78a74d17da6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I feel so unsure. As I take your hand and lead to the dance floor. As the music dies, something in your eyes. Calls to mind the silver screen. And all its sad good-byes. I\\'m never gonna dance again. Guilty feet have got no rhythm. Though it\\'s easy to pretend. I know you are not a fool. Should\\'ve known better than to cheat a friend. And waste the chance that I\\'ve been given. So I\\'m never gonna dance again. The way I danced with you. Time can never mend. The careless whispers of a good friend. To the heart and mind. Ignorance is kind. There\\'s no comfort in the truth. Pain is all you\\'ll find. I\\'m never gonna dance again. Guilty feet have got no rhythm. Though it\\'s easy to pretend. I know you are not a fool. Should\\'ve known better than to cheat a friend. And waste this chance that I\\'ve been given. So I\\'m never gonna dance again. The way I danced with you. Never without your love. Tonight the music seems so loud. I wish that we could lose this crowd. Maybe it\\'s better this way. We\\'d hurt each other. with the things we\\'d want to say. We could have been so good together. We could have lived this dance forever. But now who\\'s gonna dance with me. Please stay. And I\\'m never gonna dance again. Guilty feet have got no rhythm. Though it\\'s easy to pretend. I know you\\'re not a fool. Should\\'ve known better than to cheat a friend. And waste the chance that I\\'ve been given. So I\\'m never gonna dance again. The way I danced with you. (now that you\\'re gone) Hey ok make sure.\" [giggles as he stands up]<|endoftext|>']"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate sequences\n",
    "gen_text(df['Lyric'][0],tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xanwpp1Wy9Pr"
   },
   "outputs": [],
   "source": [
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data):\n",
    "  generated_lyrics = []\n",
    "  for i in range(len(test_data)):\n",
    "    x = gen_text(test_data['Lyric'][i], tokenizer, model)\n",
    "    generated_lyrics.append(x)\n",
    "  return generated_lyrics\n",
    "\n",
    "generated_lyrics = text_generation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghFT5K8NB_WL"
   },
   "outputs": [],
   "source": [
    "#Loop to keep only generated text and add it as a new column in the dataframe\n",
    "my_generations=[]\n",
    "\n",
    "for i in range(len(generated_lyrics)):\n",
    "  a = test_set['Lyric'][i].split()[-30:] #Get the matching string we want (30 words)\n",
    "  b = ' '.join(a)\n",
    "  c = ' '.join(generated_lyrics[i]) #Get all that comes after the matching string\n",
    "  my_generations.append(c.split(b)[-1])\n",
    "\n",
    "test_set['Generated_lyrics'] = my_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "bq6bGQTWCJ8M",
    "outputId": "b0e2047d-5d8d-43ee-e399-09dbc8204f4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>SName</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genre</th>\n",
       "      <th>True_end_lyrics</th>\n",
       "      <th>Generated_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2946</td>\n",
       "      <td>3317</td>\n",
       "      <td>Do the Clam</td>\n",
       "      <td>(Words &amp; music by Wayne - Weisman - Fuller). H...</td>\n",
       "      <td>Elvis Presley</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Grab your barefoot baby by the hand. Turn and ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12130</td>\n",
       "      <td>13349</td>\n",
       "      <td>Elevation</td>\n",
       "      <td>High, higher than the sun. You shoot me from a...</td>\n",
       "      <td>U2</td>\n",
       "      <td>Rock</td>\n",
       "      <td>in the sky. You make me feel like I can fly. S...</td>\n",
       "      <td>on earth.\\nI start reading monographs about J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>596</td>\n",
       "      <td>640</td>\n",
       "      <td>Professional Torturer</td>\n",
       "      <td>Infatuation. Court well meant. 'Cause I'm the ...</td>\n",
       "      <td>Alanis Morissette</td>\n",
       "      <td>Rock</td>\n",
       "      <td>I renounce my name. Professional torturer. I d...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3733</td>\n",
       "      <td>4116</td>\n",
       "      <td>I Am Yours</td>\n",
       "      <td>I am yours. However distant you may be. There ...</td>\n",
       "      <td>Eric Clapton</td>\n",
       "      <td>Rock</td>\n",
       "      <td>me. Each memory that has left its trace with m...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11961</td>\n",
       "      <td>13175</td>\n",
       "      <td>Bombs Away</td>\n",
       "      <td>The general scratches his belly and thinks. Hi...</td>\n",
       "      <td>The Police</td>\n",
       "      <td>Rock</td>\n",
       "      <td>hard and sweet. A military man would love to m...</td>\n",
       "      <td>straight red hair.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  ...                                   Generated_lyrics\n",
       "0     2946  ...                                                   \n",
       "1    12130  ...   on earth.\\nI start reading monographs about J...\n",
       "2      596  ...                                                   \n",
       "3     3733  ...                                                   \n",
       "4    11961  ...                                 straight red hair.\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finish the sentences when there is a point, remove after that\n",
    "final=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "  to_remove = test_set['Generated_lyrics'][i].split('.')[-1]\n",
    "  final.append(test_set['Generated_lyrics'][i].replace(to_remove,''))\n",
    "\n",
    "test_set['Generated_lyrics'] = final\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ayyjmc4COS4",
    "outputId": "53fa2df2-6ea1-4ff0-e833-fd4eeed0cf77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4075527115657135"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using BLEU score to compare the real sentences with the generated ones\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "scores=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "  reference = test_set['True_end_lyrics'][i]\n",
    "  candidate = test_set['Generated_lyrics'][i]\n",
    "  scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "statistics.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXbGFCpzCtaU",
    "outputId": "8f525b2d-406a-4ad4-ce76-8553da6e6af6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yov84tK8By9U"
   },
   "outputs": [],
   "source": [
    "#Rouge score\n",
    "from rouge import Rouge\n",
    "rouge=Rouge()\n",
    "\n",
    "rouge.get_scores(test_set['Generated_lyrics'], test_set['True_end_lyrics'], avg=True, ignore_empty=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GPT2_final",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_pytorch_env",
   "language": "python",
   "name": "my_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
