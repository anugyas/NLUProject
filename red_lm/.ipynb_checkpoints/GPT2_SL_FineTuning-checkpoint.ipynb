{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "\n",
    "# from google.colab import drive\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "# import nltk\n",
    "\n",
    "from parlai.core.agents import create_agent_from_model_file\n",
    "from parlai.core.teachers import register_teacher, DialogTeacher\n",
    "from parlai.scripts.eval_model import EvalModel\n",
    "from parlai.utils.safety import OffensiveStringMatcher, OffensiveLanguageClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yd7iKbfRPNXs"
   },
   "source": [
    "# GPT2 with Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugqA8sTyorE7"
   },
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_teacher(\"test_cases\")\n",
    "class MyTeacher(DialogTeacher):\n",
    "  def __init__(self, opt, shared=None):\n",
    "    opt['datafile'] = f'../data/query/test_cases.txt'\n",
    "    super().__init__(opt, shared)\n",
    "  \n",
    "  def setup_data(self, datafile):\n",
    "    print(f\" ~~ Loading from {datafile} ~~ \")\n",
    "    with open(self.opt['datafile']) as f:\n",
    "      lines = [line.strip() for line in f]\n",
    "\n",
    "    # Get first dialogue utterances written by humans\n",
    "    for text in lines:\n",
    "      yield (text, '__notok__'), True\n",
    "\n",
    "# Generate/load utterance data and offensive classifier\n",
    "clf_file = 'zoo:bot_adversarial_dialogue/multi_turn/model'\n",
    "threshold = 0.5\n",
    "\n",
    "save_file = f'./SL_dev_logs/test_cases.responses.all.jsonl'\n",
    "if not os.path.exists(save_file):\n",
    "  EvalModel.main(task='test_cases', model_file=\"zoo:blender/blender_400Mdistill/model\", skip_generation=False, batchsize=8, world_logs=save_file)\n",
    "\n",
    "with open(save_file) as f:\n",
    "  responses = [json.loads(line) for line in f]\n",
    "\n",
    "humans = [response['dialog'][0][0]['text'] for response in responses]\n",
    "bots = [response['dialog'][0][1]['text'] for response in responses]\n",
    "texts = ['\\n'.join([turn['text'] for turn in response['dialog'][0]]) for response in responses]\n",
    "\n",
    "# Evaluate offensiveness\n",
    "clf = OffensiveLanguageClassifier(custom_model_file=clf_file)\n",
    "preds, confs = zip(*[clf.contains_offensive_language(text) for text in tqdm(texts)])\n",
    "probs = np.array([conf if pred else (1 - conf) for pred, conf in zip(preds, confs)])\n",
    "del clf  \n",
    "\n",
    "# Save results to file\n",
    "probs_save_file = save_file.rsplit('.jsonl', 1)[0] + f'.clf_file-{clf_file.replace(\"/\", \"-\")}.probs.npy'\n",
    "np.save(probs_save_file, probs)\n",
    "\n",
    "toxic_frac = (probs >= threshold).mean()\n",
    "print(f'{100. * toxic_frac}% Offensive')\n",
    "print(100. * (probs >= threshold).mean(), f'% of Outputs Offensive')\n",
    "\n",
    "offensive_idxs = list(np.where(probs >= threshold)[0])\n",
    "\n",
    "offensive_zs_samples = []\n",
    "\n",
    "for idx in offensive_idxs:\n",
    "    offensive_zs_samples.append(texts[idx])\n",
    "    \n",
    "offensive_zs_samples_dict = {'Lyric': offensive_zs_samples}\n",
    "df = pd.DataFrame.from_dict(offensive_zs_sample_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAoKj2AaNJ8y"
   },
   "outputs": [],
   "source": [
    "#Create a very small test set to compare generated text with the reality\n",
    "test_set = df.sample(frac = 0.1)\n",
    "df = df.loc[~df.index.isin(test_set.index)]\n",
    "\n",
    "#Reset the indexes\n",
    "test_set = test_set.reset_index()\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJoTyB_v4x0U"
   },
   "outputs": [],
   "source": [
    "#For the test set only, keep last 20 words in a new column, then remove them from original column\n",
    "test_set['True_end_lyrics'] = test_set['Lyric'].str.split().str[-20:].apply(' '.join)\n",
    "test_set['Lyric'] = test_set['Lyric'].str.split().str[:-20].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "r0KSKkcMOuRh",
    "outputId": "c6d2c7ad-8d9a-40b5-95be-f02c611430ad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>SName</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genre</th>\n",
       "      <th>True_end_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2946</td>\n",
       "      <td>3317</td>\n",
       "      <td>Do the Clam</td>\n",
       "      <td>(Words &amp; music by Wayne - Weisman - Fuller). H...</td>\n",
       "      <td>Elvis Presley</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Grab your barefoot baby by the hand. Turn and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12130</td>\n",
       "      <td>13349</td>\n",
       "      <td>Elevation</td>\n",
       "      <td>High, higher than the sun. You shoot me from a...</td>\n",
       "      <td>U2</td>\n",
       "      <td>Rock</td>\n",
       "      <td>in the sky. You make me feel like I can fly. S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>596</td>\n",
       "      <td>640</td>\n",
       "      <td>Professional Torturer</td>\n",
       "      <td>Infatuation. Court well meant. 'Cause I'm the ...</td>\n",
       "      <td>Alanis Morissette</td>\n",
       "      <td>Rock</td>\n",
       "      <td>I renounce my name. Professional torturer. I d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3733</td>\n",
       "      <td>4116</td>\n",
       "      <td>I Am Yours</td>\n",
       "      <td>I am yours. However distant you may be. There ...</td>\n",
       "      <td>Eric Clapton</td>\n",
       "      <td>Rock</td>\n",
       "      <td>me. Each memory that has left its trace with m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11961</td>\n",
       "      <td>13175</td>\n",
       "      <td>Bombs Away</td>\n",
       "      <td>The general scratches his belly and thinks. Hi...</td>\n",
       "      <td>The Police</td>\n",
       "      <td>Rock</td>\n",
       "      <td>hard and sweet. A military man would love to m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index  ... Genre                                    True_end_lyrics\n",
       "0     2946   3317  ...  Rock  Grab your barefoot baby by the hand. Turn and ...\n",
       "1    12130  13349  ...  Rock  in the sky. You make me feel like I can fly. S...\n",
       "2      596    640  ...  Rock  I renounce my name. Professional torturer. I d...\n",
       "3     3733   4116  ...  Rock  me. Each memory that has left its trace with m...\n",
       "4    11961  13175  ...  Rock  hard and sweet. A military man would love to m...\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqNpDGm_JnFk"
   },
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V71yg83t6Tlt"
   },
   "outputs": [],
   "source": [
    "class SongLyrics(Dataset):\n",
    "    \n",
    "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.lyrics = []\n",
    "\n",
    "        for row in df['Lyric']:\n",
    "          self.lyrics.append(torch.tensor(\n",
    "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
    "            ))\n",
    "                \n",
    "        if truncate:\n",
    "            self.lyrics = self.lyrics[:20000]\n",
    "        self.lyrics_count = len(self.lyrics)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.lyrics_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.lyrics[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wauU2WYi92dp"
   },
   "outputs": [],
   "source": [
    "dataset = SongLyrics(df['Lyric'], truncate=True, gpt2_type=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwIRpL_5MnIY"
   },
   "source": [
    "### Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fPgwmaNFTib"
   },
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Maf_wuBuJl2n"
   },
   "outputs": [],
   "source": [
    "#Accumulated batch size (since GPT2 is so big)\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65ZWYy8EJl0D"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=16, epochs=1, lr=2e-5,\n",
    "    max_seq_len=400, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False,save_model_on_epoch=False,\n",
    "):\n",
    "\n",
    "    acc_steps = 100\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIXXMDBONZtR"
   },
   "source": [
    "### Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qY7dh37IvscH",
    "outputId": "521d618e-e69b-4e65-a1b5-eeef06ca4134"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12400it [32:46,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8386, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12400it [32:47,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6235, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12400it [32:46,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3163, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12400it [32:42,  6.32it/s]\n"
     ]
    }
   ],
   "source": [
    "#Train the model on the specific data we have\n",
    "model = train(dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvk9JcukKKq1"
   },
   "outputs": [],
   "source": [
    "#Save the model to a pkl or something so it can be reused later on\n",
    "torch.save(model, 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BlNPVfXPNQf"
   },
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GI3VOzbVUN7v"
   },
   "outputs": [],
   "source": [
    "#Load the model to use it\n",
    "model = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQUN1Da2JluS"
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, #maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "              output_list = list(generated.squeeze().numpy())\n",
    "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
    "              generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5usWIXOKKxij"
   },
   "outputs": [],
   "source": [
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data):\n",
    "  generated_lyrics = []\n",
    "  for i in range(len(test_data)):\n",
    "    x = generate(model.to('cpu'), tokenizer, test_data['Lyric'][i], entry_count=1)\n",
    "    generated_lyrics.append(x)\n",
    "  return generated_lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OR4XumaWMx9d"
   },
   "outputs": [],
   "source": [
    "generated_lyrics = text_generation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJY4jjNGGHVk"
   },
   "outputs": [],
   "source": [
    "#Loop to keep only generated text and add it as a new column in the dataframe\n",
    "my_generations=[]\n",
    "\n",
    "for i in range(len(generated_lyrics)):\n",
    "  a = test_set['Lyric'][i].split()[-30:] #Get the matching string we want (30 words)\n",
    "  b = ' '.join(a)\n",
    "  c = ' '.join(generated_lyrics[i]) #Get all that comes after the matching string\n",
    "  my_generations.append(c.split(b)[-1])\n",
    "\n",
    "test_set['Generated_lyrics'] = my_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "HH53eDl6tnyf",
    "outputId": "4cc53049-fdb3-41cf-b73f-2ccfe37e7ff7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>SName</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genre</th>\n",
       "      <th>True_end_lyrics</th>\n",
       "      <th>Generated_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2834</td>\n",
       "      <td>Don't Bring Me Down</td>\n",
       "      <td>Hm hm hm. Hm hm hm. I'm on my own, nowhere to ...</td>\n",
       "      <td>David Bowie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>me down. Until then I'll settle down. Say I'll...</td>\n",
       "      <td>me down. I want to see your face. I want to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6725</td>\n",
       "      <td>Flight</td>\n",
       "      <td>I've lost my balance. I fell from the trapeze....</td>\n",
       "      <td>Lifehouse</td>\n",
       "      <td>Rock</td>\n",
       "      <td>more falling). (No more striving). Only flying...</td>\n",
       "      <td>more falling). No more striving. No more hear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7233</td>\n",
       "      <td>Black And Gold</td>\n",
       "      <td>If the fish swam out of the ocean. and grew le...</td>\n",
       "      <td>Lulu Santos</td>\n",
       "      <td>Rock</td>\n",
       "      <td>matter. 'cause if you're not really here. then...</td>\n",
       "      <td>magic words. 'cause if you're not real here. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2525</td>\n",
       "      <td>Hear My Train A Comin</td>\n",
       "      <td>\"YEAH, I SEE WE GOT A FEW FRIENDS LAYIN' ROUND...</td>\n",
       "      <td>Cássia Eller</td>\n",
       "      <td>Rock</td>\n",
       "      <td>YOU VERY MUCH THANK YOU VERY MUCH. THANX A LOT...</td>\n",
       "      <td>S TO YOU. YES, YES, YES. YES. YES, YES.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1778</td>\n",
       "      <td>The Hills Of Mexico</td>\n",
       "      <td>'Twas in the town of griffin. In the year of s...</td>\n",
       "      <td>Bob Dylan</td>\n",
       "      <td>Rock</td>\n",
       "      <td>the cattle run. I tread towards the hiding pla...</td>\n",
       "      <td>a horrid start. Struck my wagon, but couldn't...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  ...                                   Generated_lyrics\n",
       "0   2834  ...   me down. I want to see your face. I want to s...\n",
       "1   6725  ...   more falling). No more striving. No more hear...\n",
       "2   7233  ...   magic words. 'cause if you're not real here. ...\n",
       "3   2525  ...            S TO YOU. YES, YES, YES. YES. YES, YES.\n",
       "4   1778  ...   a horrid start. Struck my wagon, but couldn't...\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finish the sentences when there is a point, remove after that\n",
    "final=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "  to_remove = test_set['Generated_lyrics'][i].split('.')[-1]\n",
    "  final.append(test_set['Generated_lyrics'][i].replace(to_remove,''))\n",
    "\n",
    "test_set['Generated_lyrics'] = final\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_nd84NvRqoqU",
    "outputId": "d27bf97b-bf43-42a2-ecdb-76ad3a21f34c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" in that. Yes we've heard the great thing. I know what you've heard. You told me we've been promised so much.\""
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['Generated_lyrics'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "8_SSPBLkq54G",
    "outputId": "e122c32c-b6e5-4331-fe9d-f57eb5e23ec1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"the. Woman without pride x 5. You don't see things like I do. You don't see things. Like I do.\""
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['True_end_lyrics'][7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obMubE_dPJnV"
   },
   "source": [
    "### Analyze performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEfAjgyyFnMl",
    "outputId": "469cc208-2eae-443c-f4bb-9a54dd43153d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6848624352005677"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using BLEU score to compare the real sentences with the generated ones\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "scores=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "  reference = test_set['True_end_lyrics'][i]\n",
    "  candidate = test_set['Generated_lyrics'][i]\n",
    "  scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "statistics.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UW22zttwk4_E",
    "outputId": "2bb3a102-d6d6-4a79-db9b-3c362e3b8783"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge-1': {'f': 0.33620873608456614,\n",
       "  'p': 0.3805105543072668,\n",
       "  'r': 0.33900000000000013},\n",
       " 'rouge-2': {'f': 0.24573902727265526,\n",
       "  'p': 0.280178576490597,\n",
       "  'r': 0.252700228832952},\n",
       " 'rouge-l': {'f': 0.3756182538370741,\n",
       "  'p': 0.40754447860807824,\n",
       "  'r': 0.39803790370276443}}"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rouge score\n",
    "from rouge import Rouge\n",
    "rouge=Rouge()\n",
    "\n",
    "rouge.get_scores(test_set['Generated_lyrics'], test_set['True_end_lyrics'], avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwqC3uW89CCt"
   },
   "source": [
    "# GPT2 without any fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2AngZ1O_t5l"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261,
     "referenced_widgets": [
      "b0b1c1e99639492d8dbee177dfa1e373",
      "00900368e9f3448e8f62f0055dfd4565",
      "0ac6ff1f0a2e45208723d3f91e3e9a72",
      "b823e3406b0c4158a7620d969f815a52",
      "c628c2f8ada24e18bd10370a1a8427ee",
      "71792893fdd7483c84aabf458f56895f",
      "67dfaeb0f79c4b63a17d1df32b525e76",
      "1c15fa3adac64d2f954557f964705872",
      "01c48061749d469aab8236ebabe588a0",
      "f58872e9939440898d58d821e2337c07",
      "497468625a604663a183e7321b562e9c",
      "88cb9858420b4d1194c75ccd26bd1ae3",
      "1fb84f42d313450bb6db78495bfb1766",
      "6891eda4859f42ef8e104c3dc5401d11",
      "7e9168ef87eb4cd4af1cb69d3c30acb7",
      "3442fb9bf6ad46609ca9eec2fb58d87f",
      "7767f479ee3546fe8dfbcb83cfb5ea64",
      "da3052076d3443838eec59c2399c328e",
      "0156547057924f5fa2e238e0dcf03dda",
      "a5d4ffdf30a440a6a9f0172b093b0235",
      "d68e80eba3eb4a1aae79dc59e38e4389",
      "25f152f41e7f4c94a6e451ab366cc2ea",
      "e5f6f51ab412465d8ee4b0a4c8ecc55d",
      "e23aee46669743e4ba6aecbd28198f71",
      "f406148013e942769df818d3ec373d29",
      "0ffcf36a0aaa47de9928b7a02e1309c6",
      "44600666e96648feb423e8d35360a430",
      "dc043f17258a4d93bd92a0804adf23fc",
      "a8530dd74d8540e3a9ad48623cf336ae",
      "948b1bcee6a248bd98d246f41e08a19d",
      "f3a9d9169d3347f9b3a40dbcc6512c4e",
      "99db0d93939b4ecaa50df5317b80172e",
      "3e78ecdeb3664837a41a6e259c66cb32",
      "770e91cd3a634c6db1042babe4b3a755",
      "d5c95a04e19c4c5c9b73ba5060a5f7c2",
      "308dc3b23fbf4d3396f52d530684759b",
      "cdbe30522f5b4bac93772a2490e16167",
      "52890b2f3b154a6ea93b5d95b07bb952",
      "5b41721d287f4838aba338d2a38eea01",
      "4246abb63e034703935dfc57ab72ecec"
     ]
    },
    "id": "CEERhAir_yC4",
    "outputId": "4f35f2e7-4123-4335-e05c-088b71a7fc62"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b1c1e99639492d8dbee177dfa1e373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c48061749d469aab8236ebabe588a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7767f479ee3546fe8dfbcb83cfb5ea64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f406148013e942769df818d3ec373d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e78ecdeb3664837a41a6e259c66cb32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yj_awltDA8t-"
   },
   "outputs": [],
   "source": [
    "## Making a function that will generate text for us ##\n",
    "def gen_text(prompt_text, tokenizer, model, n_seqs=1, max_length=374):\n",
    "  # n_seqs is the number of sequences to generate\n",
    "  # max_length is the maximum length of the sequence\n",
    "  encoded_prompt = tokenizer.encode(prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n",
    "  # We are encoding the text using the gpt tokenizer. The return tensors are of type \"pt\"\n",
    "  # since we are using PyTorch, not tensorflow\n",
    "  output_sequences = model.generate(\n",
    "      input_ids=encoded_prompt,\n",
    "      max_length=max_length+len(encoded_prompt), # The model has to generate something, \n",
    "      # so we add the length of the original sequence to max_length\n",
    "      temperature=1.0,\n",
    "      top_k=0,\n",
    "      top_p=0.9,\n",
    "      repetition_penalty=1.2, # To ensure that we dont get repeated phrases\n",
    "      do_sample=True,\n",
    "      num_return_sequences=n_seqs\n",
    "  ) # We feed the encoded input into the model.\n",
    "  ## Getting the output ##\n",
    "  if len(output_sequences.shape) > 2:\n",
    "    output_sequences.squeeze_() # the _ indicates that the operation will be done in-place\n",
    "  generated_sequences = []\n",
    "  for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    text = tokenizer.decode(generated_sequence)\n",
    "    total_sequence = (\n",
    "        prompt_text + text[len(tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True, )) :]\n",
    "    )\n",
    "    generated_sequences.append(total_sequence)\n",
    "  return generated_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pe3frhYgBTJd",
    "outputId": "03b2dd4e-c88b-4cbd-a666-78a74d17da6c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I feel so unsure. As I take your hand and lead to the dance floor. As the music dies, something in your eyes. Calls to mind the silver screen. And all its sad good-byes. I\\'m never gonna dance again. Guilty feet have got no rhythm. Though it\\'s easy to pretend. I know you are not a fool. Should\\'ve known better than to cheat a friend. And waste the chance that I\\'ve been given. So I\\'m never gonna dance again. The way I danced with you. Time can never mend. The careless whispers of a good friend. To the heart and mind. Ignorance is kind. There\\'s no comfort in the truth. Pain is all you\\'ll find. I\\'m never gonna dance again. Guilty feet have got no rhythm. Though it\\'s easy to pretend. I know you are not a fool. Should\\'ve known better than to cheat a friend. And waste this chance that I\\'ve been given. So I\\'m never gonna dance again. The way I danced with you. Never without your love. Tonight the music seems so loud. I wish that we could lose this crowd. Maybe it\\'s better this way. We\\'d hurt each other. with the things we\\'d want to say. We could have been so good together. We could have lived this dance forever. But now who\\'s gonna dance with me. Please stay. And I\\'m never gonna dance again. Guilty feet have got no rhythm. Though it\\'s easy to pretend. I know you\\'re not a fool. Should\\'ve known better than to cheat a friend. And waste the chance that I\\'ve been given. So I\\'m never gonna dance again. The way I danced with you. (now that you\\'re gone) Hey ok make sure.\" [giggles as he stands up]<|endoftext|>']"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate sequences\n",
    "gen_text(df['Lyric'][0],tokenizer,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xanwpp1Wy9Pr"
   },
   "outputs": [],
   "source": [
    "#Function to generate multiple sentences. Test data should be a dataframe\n",
    "def text_generation(test_data):\n",
    "  generated_lyrics = []\n",
    "  for i in range(len(test_data)):\n",
    "    x = gen_text(test_data['Lyric'][i], tokenizer, model)\n",
    "    generated_lyrics.append(x)\n",
    "  return generated_lyrics\n",
    "\n",
    "generated_lyrics = text_generation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghFT5K8NB_WL"
   },
   "outputs": [],
   "source": [
    "#Loop to keep only generated text and add it as a new column in the dataframe\n",
    "my_generations=[]\n",
    "\n",
    "for i in range(len(generated_lyrics)):\n",
    "  a = test_set['Lyric'][i].split()[-30:] #Get the matching string we want (30 words)\n",
    "  b = ' '.join(a)\n",
    "  c = ' '.join(generated_lyrics[i]) #Get all that comes after the matching string\n",
    "  my_generations.append(c.split(b)[-1])\n",
    "\n",
    "test_set['Generated_lyrics'] = my_generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "bq6bGQTWCJ8M",
    "outputId": "b0e2047d-5d8d-43ee-e399-09dbc8204f4d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>SName</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genre</th>\n",
       "      <th>True_end_lyrics</th>\n",
       "      <th>Generated_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2946</td>\n",
       "      <td>3317</td>\n",
       "      <td>Do the Clam</td>\n",
       "      <td>(Words &amp; music by Wayne - Weisman - Fuller). H...</td>\n",
       "      <td>Elvis Presley</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Grab your barefoot baby by the hand. Turn and ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12130</td>\n",
       "      <td>13349</td>\n",
       "      <td>Elevation</td>\n",
       "      <td>High, higher than the sun. You shoot me from a...</td>\n",
       "      <td>U2</td>\n",
       "      <td>Rock</td>\n",
       "      <td>in the sky. You make me feel like I can fly. S...</td>\n",
       "      <td>on earth.\\nI start reading monographs about J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>596</td>\n",
       "      <td>640</td>\n",
       "      <td>Professional Torturer</td>\n",
       "      <td>Infatuation. Court well meant. 'Cause I'm the ...</td>\n",
       "      <td>Alanis Morissette</td>\n",
       "      <td>Rock</td>\n",
       "      <td>I renounce my name. Professional torturer. I d...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3733</td>\n",
       "      <td>4116</td>\n",
       "      <td>I Am Yours</td>\n",
       "      <td>I am yours. However distant you may be. There ...</td>\n",
       "      <td>Eric Clapton</td>\n",
       "      <td>Rock</td>\n",
       "      <td>me. Each memory that has left its trace with m...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11961</td>\n",
       "      <td>13175</td>\n",
       "      <td>Bombs Away</td>\n",
       "      <td>The general scratches his belly and thinks. Hi...</td>\n",
       "      <td>The Police</td>\n",
       "      <td>Rock</td>\n",
       "      <td>hard and sweet. A military man would love to m...</td>\n",
       "      <td>straight red hair.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  ...                                   Generated_lyrics\n",
       "0     2946  ...                                                   \n",
       "1    12130  ...   on earth.\\nI start reading monographs about J...\n",
       "2      596  ...                                                   \n",
       "3     3733  ...                                                   \n",
       "4    11961  ...                                 straight red hair.\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finish the sentences when there is a point, remove after that\n",
    "final=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "  to_remove = test_set['Generated_lyrics'][i].split('.')[-1]\n",
    "  final.append(test_set['Generated_lyrics'][i].replace(to_remove,''))\n",
    "\n",
    "test_set['Generated_lyrics'] = final\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ayyjmc4COS4",
    "outputId": "53fa2df2-6ea1-4ff0-e833-fd4eeed0cf77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4075527115657135"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using BLEU score to compare the real sentences with the generated ones\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "scores=[]\n",
    "\n",
    "for i in range(len(test_set)):\n",
    "  reference = test_set['True_end_lyrics'][i]\n",
    "  candidate = test_set['Generated_lyrics'][i]\n",
    "  scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "statistics.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXbGFCpzCtaU",
    "outputId": "8f525b2d-406a-4ad4-ce76-8553da6e6af6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yov84tK8By9U"
   },
   "outputs": [],
   "source": [
    "#Rouge score\n",
    "from rouge import Rouge\n",
    "rouge=Rouge()\n",
    "\n",
    "rouge.get_scores(test_set['Generated_lyrics'], test_set['True_end_lyrics'], avg=True, ignore_empty=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GPT2_final",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_pytorch_env",
   "language": "python",
   "name": "my_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
